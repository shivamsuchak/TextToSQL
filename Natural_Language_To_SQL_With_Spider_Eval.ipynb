{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "mIsUsrZKWCTW"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Natural Language to SQL"
      ],
      "metadata": {
        "id": "B48xpYOXarWH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "from datetime import date, timedelta\n",
        "from sqlalchemy.orm import declarative_base, relationship, sessionmaker\n",
        "from sqlalchemy import create_engine, Column, Integer, String, ForeignKey, Date, Float\n",
        "import sqlite3\n",
        "import contextlib"
      ],
      "metadata": {
        "id": "FjRbFQMLoUSK"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -U -q langchain-community langchain-core langgraph langchain-groq"
      ],
      "metadata": {
        "id": "RluXPgQQcGLh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80a538f1-0c56-4486-83fd-27b60d8f5b0a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.3/438.3 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.9/154.9 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.4/129.4 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.2/44.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.0/50.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.5/216.5 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "from google.colab import userdata\n",
        "\n",
        "#os.environ['GROQ_API_KEY'] = userdata.get('GROQ_API_KEY')\n",
        "os.environ['GROQ_API_KEY'] = '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'\n",
        "\n",
        "\n",
        "llm = ChatGroq(model='llama-3.3-70b-versatile')\n",
        "\n",
        "print(llm.invoke('who are you?').content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "MCKXmZ5xkzFZ",
        "outputId": "356050e4-ebcb-4c97-b8ec-fe01054bbe29"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "BadRequestError",
          "evalue": "Error code: 400 - {'error': {'message': 'Organization has been restricted. Please reach out to support if you believe this was in error.', 'type': 'invalid_request_error', 'code': 'organization_restricted'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-005bc3c9d9af>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mllm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChatGroq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'llama-3.3-70b-versatile'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'who are you?'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    369\u001b[0m         return cast(\n\u001b[1;32m    370\u001b[0m             \u001b[0;34m\"ChatGeneration\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m             self.generate_prompt(\n\u001b[0m\u001b[1;32m    372\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    954\u001b[0m     ) -> LLMResult:\n\u001b[1;32m    955\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 956\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    957\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverride\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    773\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m                 results.append(\n\u001b[0;32m--> 775\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    776\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1019\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_from_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m             result = self._generate(\n\u001b[0m\u001b[1;32m   1022\u001b[0m                 \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_groq/chat_models.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    496\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         }\n\u001b[0;32m--> 498\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessage_dicts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_chat_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/groq/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, exclude_domains, frequency_penalty, function_call, functions, include_domains, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, n, parallel_tool_calls, presence_penalty, reasoning_format, response_format, search_settings, seed, service_tier, stop, store, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    359\u001b[0m           \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOverride\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlevel\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m         \"\"\"\n\u001b[0;32m--> 361\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    362\u001b[0m             \u001b[0;34m\"/openai/v1/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1220\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m         )\n\u001b[0;32m-> 1222\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1029\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1031\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': 'Organization has been restricted. Please reach out to support if you believe this was in error.', 'type': 'invalid_request_error', 'code': 'organization_restricted'}}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.utilities.sql_database import SQLDatabase\n",
        "\n",
        "db = SQLDatabase.from_uri(\"sqlite:///example.db\")\n",
        "\n",
        "print(db.dialect)\n",
        "\n",
        "print(\"=\" * 30, \"\\n\")\n",
        "print(db.get_usable_table_names())\n",
        "print(\"=\" * 30, \"\\n\")\n",
        "print(db.get_table_info())"
      ],
      "metadata": {
        "id": "m6NKM5oBly8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing_extensions import TypedDict\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "\n",
        "## Initial state\n",
        "class AgentState(TypedDict):\n",
        "    question: str\n",
        "    sql_query: str\n",
        "    query_result: str\n",
        "    query_rows: list\n",
        "    attempts: int\n",
        "    relevance: str\n",
        "    final_answer: str\n",
        "    sql_error: bool"
      ],
      "metadata": {
        "id": "pU9nCdDnsrbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Agent 1: Determines if the user NL-input relevant to the db schema\n",
        "\n",
        "class CheckRelevance(BaseModel):\n",
        "    relevance: str = Field(\n",
        "        description=\"Indicates whether the question is related to the database schema. 'relevant' or 'not_relevant'.\"\n",
        "    )\n",
        "\n",
        "def check_relevance(state: AgentState):\n",
        "    question = state[\"question\"]\n",
        "    print(f\"Checking relevance of the question: {question}\")\n",
        "    system = \"\"\"You are a highly skilled SQL expert. Your task is to evaluate whether a given natural language question is relevant to a database based on its schema.\n",
        "\n",
        "    Use the schema below to determine whether the question can reasonably be answered using the available tables and columns.\n",
        "\n",
        "    ---\n",
        "    Schema:\n",
        "    {schema}\n",
        "    ---\n",
        "\n",
        "    Instructions:\n",
        "    1. Carefully read the user's question.\n",
        "    2. Check whether the schema contains the necessary tables and columns to answer the question.\n",
        "    3. If the question can be answered using the schema, respond with **\"relevant\"**.\n",
        "    4. If the schema lacks the necessary information, respond with **\"not_relevant\"**.\n",
        "    5. Your response must be either **\"relevant\"** or **\"not_relevant\"** only—do not explain or elaborate.\n",
        "\n",
        "    Output Format:\n",
        "    relevant\n",
        "    or\n",
        "    not_relevant\n",
        "    \"\"\"\n",
        "    human = f\"Question: {question}\"\n",
        "    check_prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", system),\n",
        "            (\"human\", human),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    llm = ChatGroq(model='llama-3.3-70b-versatile')\n",
        "    structured_llm = llm.with_structured_output(CheckRelevance)\n",
        "    relevance_checker = check_prompt | structured_llm\n",
        "    try:\n",
        "      relevance = relevance_checker.invoke({'schema': db.get_table_info()})\n",
        "      state[\"relevance\"] = relevance.relevance.lower().strip()\n",
        "      print(f\"Relevance determined: {state['relevance']}\")\n",
        "    except Exception as e:\n",
        "      print(f\"Error during relevance check: {e}\")\n",
        "      state[\"relevance\"] = \"not_relevant\"\n",
        "      state[\"sql_error\"] = True\n",
        "    return state"
      ],
      "metadata": {
        "id": "zcx40Ie9NlHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Agent 2: Converts natural language to SQL queries\n",
        "\n",
        "class ConvertToSQL(BaseModel):\n",
        "    sql_query: str = Field(\n",
        "        description=\"The SQL query corresponding to the user's natural language question.\"\n",
        "    )\n",
        "\n",
        "\n",
        "def convert_nl_to_sql(state: AgentState):\n",
        "    question = state[\"question\"]\n",
        "    system = \"\"\"\n",
        "    You are a highly skilled SQL generation assistant. Your task is to convert a user's natural language question into a valid, syntactically correct, and semantically meaningful SQL query using the correct {dialect} dialect.\n",
        "\n",
        "    Follow these strict rules:\n",
        "\n",
        "    1. **Use only the tables and columns provided in the schema below**. Do not invent or reference tables/columns that are not explicitly listed.\n",
        "    2. **Understand the relationships between tables** (e.g., foreign keys, primary keys) and use JOINs accordingly where appropriate.\n",
        "    3. **Avoid using `SELECT *`**. Instead, return only the specific columns that are relevant to answering the user's question.\n",
        "    4. Use appropriate **filters, sorting, and grouping** based on the user's intent (e.g., time ranges, categories, totals).\n",
        "    5. If necessary, use **aggregations** (COUNT, AVG, MAX, etc.) when the question asks for summaries or statistics.\n",
        "    6. Maintain clarity and simplicity. Prioritize correctness over cleverness.\n",
        "\n",
        "    Before generating the SQL:\n",
        "    - Carefully analyze the user's question.\n",
        "    - Infer any implicit intent (e.g., filtering, ordering) only if it logically follows from the question.\n",
        "    - Never assume facts that are not supported by the schema or the question.\n",
        "\n",
        "    Schema:\n",
        "    {table_info}\n",
        "\n",
        "    Now, generate the SQL query that answers the following user question:\n",
        "    \"\"\".format(dialect=db.dialect, table_info=db.get_table_info())\n",
        "\n",
        "    convert_prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", system),\n",
        "            (\"human\", \"Question: {question}\"),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    llm = ChatGroq(model='llama-3.3-70b-versatile')\n",
        "    structured_llm = llm.with_structured_output(ConvertToSQL)\n",
        "    sql_generator = convert_prompt | structured_llm\n",
        "    try:\n",
        "      result = sql_generator.invoke({\"question\": question})\n",
        "      state[\"sql_query\"] = result.sql_query.strip()\n",
        "    except Exception as e:\n",
        "      print(f\"Failed to generate SQL: {e}\")\n",
        "      state[\"sql_query\"] = \"\"\n",
        "      state[\"sql_error\"] = True\n",
        "    return state"
      ],
      "metadata": {
        "id": "foWES7lUyoqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.tools import QuerySQLDatabaseTool\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "## Agent 3: Executes generated SQL query -> retrieve the data from db -> returns updated state\n",
        "def execute_query(state: AgentState):\n",
        "    \"\"\"Execute SQL query and update state based on outcome.\"\"\"\n",
        "    execute_query_tool = QuerySQLDatabaseTool(db=db)\n",
        "    try:\n",
        "        result = execute_query_tool.invoke(state[\"sql_query\"])\n",
        "        state[\"query_result\"] = result\n",
        "        state[\"sql_error\"] = False\n",
        "    except Exception as e:\n",
        "        print(f\"Error during SQL execution: {e}\")\n",
        "        state[\"sql_error\"] = True\n",
        "        state[\"query_result\"] = str(e)\n",
        "    return state"
      ],
      "metadata": {
        "id": "jIuyUPk3P52F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Agent 4: Based on SQL result, generates NL answer\n",
        "def generate_answer(state: AgentState):\n",
        "    \"\"\"Answer question using retrieved information as context.\"\"\"\n",
        "    template = PromptTemplate.from_template(\n",
        "    \"\"\"You are an intelligent data assistant. Your task is to help answer the user's natural language question by using the provided SQL query and its result.\n",
        "\n",
        "    You will be given:\n",
        "    1. The original user question.\n",
        "    2. The SQL query that was generated to answer the question.\n",
        "    3. The result returned by executing that SQL query.\n",
        "\n",
        "    Use this information to provide a helpful, clear, and concise answer to the user's question. If the result is empty or insufficient to answer the question confidently, respond accordingly.\n",
        "\n",
        "    ---\n",
        "    Question: {question}\n",
        "    SQL Query: {sql_query}\n",
        "    SQL Result: {query_result}\n",
        "    ---\n",
        "\n",
        "    Final Answer:\"\"\")\n",
        "\n",
        "    llm_chain = template | llm | StrOutputParser()\n",
        "    answer = llm_chain.invoke({\n",
        "        \"question\": state[\"question\"],\n",
        "        \"sql_query\": state[\"sql_query\"],\n",
        "        \"query_result\": state[\"query_result\"]\n",
        "    })\n",
        "    state[\"final_answer\"] = answer\n",
        "    return state"
      ],
      "metadata": {
        "id": "7roEgAdPOuyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Agent 5: Generates funny response if the user's query is not relevent to the db\n",
        "def generate_funny_response(state: AgentState):\n",
        "    print(\"Generating a funny response for an unrelated question.\")\n",
        "    system = \"\"\"\n",
        "    You are a witty, charming, and funny assistant whose job is to entertain users when they ask questions unrelated to the database or when no relevant answer can be provided.\n",
        "\n",
        "    Your responses should:\n",
        "    - Be playful and light-hearted.\n",
        "    - Stay appropriate and friendly.\n",
        "    - Acknowledge that the question isn't answerable via the database.\n",
        "    - Gently steer the user back on track with a smile (figuratively).\n",
        "\n",
        "    You are not required to be helpful — just be delightfully unhelpful in a clever way.\n",
        "    \"\"\"\n",
        "\n",
        "    human_message = f\"\"\"\n",
        "    The user asked a question that is unrelated to the database:\n",
        "    '{state['question']}'\n",
        "    Craft a humorous and creative response.\"\"\"\n",
        "\n",
        "    funny_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", system),\n",
        "        (\"human\", human_message),\n",
        "    ])\n",
        "\n",
        "    llm = ChatGroq(model='llama-3.3-70b-versatile')\n",
        "    funny_response = funny_prompt | llm | StrOutputParser()\n",
        "    message = funny_response.invoke({})\n",
        "    state[\"final_answer\"] = message\n",
        "    print(\"Generated funny response.\")\n",
        "    return state"
      ],
      "metadata": {
        "id": "7_eHvGZmQi12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Agent 6: Rewrites the og question if there isn't enough info\n",
        "class RewrittenQuestion(BaseModel):\n",
        "    question: str = Field(description=\"The rewritten question.\")\n",
        "\n",
        "def regenerate_query(state: AgentState):\n",
        "    question = state[\"question\"]\n",
        "    print(\"Regenerating the SQL query by rewriting the question.\")\n",
        "    system = \"\"\"\n",
        "    You are an expert in SQL and natural language understanding.\n",
        "\n",
        "    Your task is to **rewrite a user's natural language question** so that:\n",
        "    - It is clear, complete, and unambiguous.\n",
        "    - It is optimized to be converted into a precise and valid SQL query.\n",
        "    - All necessary details (e.g. filters, relationships between tables, required joins, and any implied logic) are included.\n",
        "    - The reformulated version preserves the intent and meaning of the original question but improves its structure for programmatic interpretation.\n",
        "\n",
        "    Avoid making assumptions not supported by the original question or schema.\n",
        "    \"\"\"\n",
        "\n",
        "    rewrite_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"Original Question: {question}\\n\\nRewrite this question to make it clearer and more suitable for SQL generation, including all relevant details.\")\n",
        "    ])\n",
        "\n",
        "    llm = ChatGroq(model='llama-3.3-70b-versatile', temperature=0)\n",
        "    structured_llm = llm.with_structured_output(RewrittenQuestion)\n",
        "    rewriter = rewrite_prompt | structured_llm\n",
        "\n",
        "    rewritten = rewriter.invoke({\"question\": question})\n",
        "    state[\"rewritten_question\"] = rewritten.question\n",
        "    state[\"attempts\"] = state.get(\"attempts\", 0) + 1\n",
        "    print(f\"Rewritten question: {state['rewritten_question']}\")\n",
        "    return state"
      ],
      "metadata": {
        "id": "d4qoYYMaPidN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Conditionnal nodes\n",
        "def end_max_iter(state: AgentState):\n",
        "    print(\"Maximum attempts reached. Ending the workflow.\")\n",
        "    state[\"query_result\"] = \"Please try again.\"\n",
        "    return state\n",
        "\n",
        "def router(state: AgentState):\n",
        "    print(\"Routing based on relevance...\")\n",
        "    if state[\"relevance\"].lower() == \"relevant\":\n",
        "        return \"convert_to_sql\"\n",
        "    else:\n",
        "        return \"generate_funny_response\"\n",
        "\n",
        "def check_attempts(state: AgentState):\n",
        "    print(f\"Attempt #{state['attempts']}\")\n",
        "    if state[\"attempts\"] < 3:\n",
        "        return \"convert_to_sql\"\n",
        "    else:\n",
        "        return \"end_max_iter\"\n",
        "\n",
        "def execute_sql(state: AgentState):\n",
        "    print(\"Routing based on SQL execution result...\")\n",
        "    if not state.get(\"sql_error\", False):\n",
        "        return \"generate_answer\"\n",
        "    else:\n",
        "        return \"regenerate_query\""
      ],
      "metadata": {
        "id": "l8cMGwpGSMp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Constructing the the Graph\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "# Add nodes\n",
        "workflow.add_node(\"check_relevance\", check_relevance)\n",
        "workflow.add_node(\"convert_to_sql\", convert_nl_to_sql)\n",
        "workflow.add_node(\"execute_sql\", execute_query)\n",
        "workflow.add_node(\"generate_answer\", generate_answer)\n",
        "workflow.add_node(\"generate_funny_response\", generate_funny_response)\n",
        "workflow.add_node(\"regenerate_query\", regenerate_query)\n",
        "workflow.add_node(\"end_max_iter\", end_max_iter)\n",
        "\n",
        "# Conditional logic\n",
        "workflow.add_conditional_edges(\n",
        "    \"check_relevance\",\n",
        "    router,\n",
        "    {\n",
        "        \"convert_to_sql\": \"convert_to_sql\",\n",
        "        \"generate_funny_response\": \"generate_funny_response\",\n",
        "    },\n",
        ")\n",
        "\n",
        "workflow.add_edge(\"convert_to_sql\", \"execute_sql\")\n",
        "\n",
        "workflow.add_conditional_edges(\n",
        "    \"execute_sql\",\n",
        "    execute_sql,\n",
        "    {\n",
        "        \"generate_answer\": \"generate_answer\",\n",
        "        \"regenerate_query\": \"regenerate_query\",\n",
        "    },\n",
        ")\n",
        "\n",
        "workflow.add_conditional_edges(\n",
        "    \"regenerate_query\",\n",
        "    check_attempts,\n",
        "    {\n",
        "        \"convert_to_sql\": \"convert_to_sql\",\n",
        "        \"end_max_iter\": \"end_max_iter\",\n",
        "    },\n",
        ")\n",
        "\n",
        "# Terminal paths\n",
        "workflow.add_edge(\"generate_answer\", END)\n",
        "workflow.add_edge(\"generate_funny_response\", END)\n",
        "workflow.add_edge(\"end_max_iter\", END)\n",
        "\n",
        "# Start point\n",
        "workflow.set_entry_point(\"check_relevance\")\n",
        "\n",
        "# Compile\n",
        "app = workflow.compile()"
      ],
      "metadata": {
        "id": "5v9c6Ol7SVR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "try:\n",
        "  display(Image(app.get_graph(xray=True).draw_mermaid_png(max_retries=5, retry_delay=2.0)))\n",
        "except:\n",
        "  pass"
      ],
      "metadata": {
        "id": "wKdYgNy-1_Sc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# display(Image(app.get_graph(xray=True).draw_mermaid_png(max_retries=5, retry_delay=2.0)))"
      ],
      "metadata": {
        "id": "TOl_qE0mc6-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# Initialize state\n",
        "state = {\n",
        "    \"question\": \"\",\n",
        "    \"chat_history\": [], # adding memory\n",
        "    \"sql_query\": \"\",\n",
        "    \"query_result\": \"\",\n",
        "    \"query_rows\": [],\n",
        "    \"attempts\": 0,\n",
        "    \"relevance\": \"\",\n",
        "    \"final_answer\": \"\",\n",
        "    \"sql_error\": False,\n",
        "}\n",
        "\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"User: \").strip()\n",
        "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
        "        print(\"Conversation ended.\")\n",
        "        break\n",
        "\n",
        "    state[\"question\"] = user_input\n",
        "    state[\"attempts\"] = 0  # reset attempts each new question\n",
        "\n",
        "    result = app.invoke(state)\n",
        "\n",
        "    answer = result.get(\"final_answer\", \"No response available.\")\n",
        "\n",
        "    print(f\"Assistant: {answer}\\n\")\n",
        "\n",
        "    state[\"chat_history\"].append({\"user\": user_input, \"assistant\": answer})\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "4soyO6Yt_8S7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import sqlparse\n",
        "import sqlite3\n",
        "import json\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "def load_spider_dev(json_path):\n",
        "    with open(json_path, \"r\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def load_dev_gold(sql_path):\n",
        "    with open(sql_path, \"r\") as f:\n",
        "        return [line.strip().split('\\t')[0] for line in f if line.strip()]\n",
        "\n",
        "def get_tables_in_db(conn):\n",
        "    cursor = conn.cursor()\n",
        "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "    tables = set(row[0].lower() for row in cursor.fetchall())\n",
        "    cursor.close()\n",
        "    return tables\n",
        "\n",
        "def fix_table_names_in_sql(sql, existing_tables):\n",
        "    def replacement(match):\n",
        "        tbl = match.group(0)\n",
        "        if tbl.lower() in existing_tables:\n",
        "            return tbl\n",
        "        if tbl.lower().endswith('s'):\n",
        "            singular = tbl[:-1]\n",
        "            if singular.lower() in existing_tables:\n",
        "                return singular\n",
        "        return tbl\n",
        "\n",
        "    pattern = re.compile(r\"(?<=FROM\\s)(\\w+)|(?<=JOIN\\s)(\\w+)\", re.IGNORECASE)\n",
        "    return pattern.sub(replacement, sql)\n",
        "\n",
        "def normalize_sql(sql):\n",
        "    sql = sql.strip().rstrip(';')\n",
        "    parsed = sqlparse.format(sql, keyword_case='lower', strip_comments=True, reindent=True)\n",
        "    return \" \".join(parsed.strip().split()).lower()\n",
        "\n",
        "def normalize_result(result):\n",
        "    if result is None:\n",
        "        return None\n",
        "    return set(tuple(str(item).strip() for item in row) for row in result)\n",
        "\n",
        "def validate_sql(sql, conn):\n",
        "    try:\n",
        "        conn.execute(\"EXPLAIN QUERY PLAN \" + sql)\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        return False\n",
        "\n",
        "def execute_sql_direct(sql, conn):\n",
        "    try:\n",
        "        cursor = conn.cursor()\n",
        "        cursor.execute(sql)\n",
        "        result = cursor.fetchall()\n",
        "        cursor.close()\n",
        "        return result\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def execute_query(state, conn):\n",
        "    try:\n",
        "        cursor = conn.cursor()\n",
        "        cursor.execute(state[\"sql_query\"])\n",
        "        result = cursor.fetchall()\n",
        "        cursor.close()\n",
        "        state[\"query_result\"] = result\n",
        "        state[\"sql_error\"] = False\n",
        "    except Exception:\n",
        "        state[\"query_result\"] = None\n",
        "        state[\"sql_error\"] = True\n",
        "    return state\n",
        "\n",
        "def extract_components(sql):\n",
        "    parsed = sqlparse.parse(sql)\n",
        "    if not parsed:\n",
        "        return {\"select\": set(), \"from\": set(), \"where\": set()}\n",
        "    stmt = parsed[0]\n",
        "\n",
        "    select_tokens = set()\n",
        "    from_tokens = set()\n",
        "    where_tokens = set()\n",
        "\n",
        "    is_select = False\n",
        "    is_from = False\n",
        "    is_where = False\n",
        "\n",
        "    for token in stmt.tokens:\n",
        "        if token.is_group:\n",
        "            for subtoken in token.flatten():\n",
        "                tval = subtoken.value.lower().strip()\n",
        "                if tval in (\"select\", \"from\", \"where\"):\n",
        "                    is_select = tval == \"select\"\n",
        "                    is_from = tval == \"from\"\n",
        "                    is_where = tval == \"where\"\n",
        "                    continue\n",
        "                if is_select and subtoken.ttype in (sqlparse.tokens.Name, sqlparse.tokens.Wildcard):\n",
        "                    select_tokens.add(tval)\n",
        "                elif is_from and subtoken.ttype in (sqlparse.tokens.Name,):\n",
        "                    from_tokens.add(tval)\n",
        "                elif is_where and subtoken.ttype in (sqlparse.tokens.Name, sqlparse.tokens.Literal.Number.Integer, sqlparse.tokens.Operator.Comparison):\n",
        "                    where_tokens.add(tval)\n",
        "        else:\n",
        "            tval = token.value.lower().strip()\n",
        "            if tval in (\"select\", \"from\", \"where\"):\n",
        "                is_select = tval == \"select\"\n",
        "                is_from = tval == \"from\"\n",
        "                is_where = tval == \"where\"\n",
        "                continue\n",
        "            if is_select and token.ttype in (sqlparse.tokens.Name, sqlparse.tokens.Wildcard):\n",
        "                select_tokens.add(tval)\n",
        "            elif is_from and token.ttype in (sqlparse.tokens.Name,):\n",
        "                from_tokens.add(tval)\n",
        "            elif is_where and token.ttype in (sqlparse.tokens.Name, sqlparse.tokens.Literal.Number.Integer, sqlparse.tokens.Operator.Comparison):\n",
        "                where_tokens.add(tval)\n",
        "\n",
        "    return {\"select\": select_tokens, \"from\": from_tokens, \"where\": where_tokens}\n",
        "\n",
        "def jaccard_similarity(set1, set2):\n",
        "    if not set1 and not set2:\n",
        "        return 1.0\n",
        "    intersection = len(set1.intersection(set2))\n",
        "    union = len(set1.union(set2))\n",
        "    return intersection / union if union > 0 else 0\n",
        "\n",
        "def string_similarity(a, b):\n",
        "    return SequenceMatcher(None, a, b).ratio()\n",
        "\n",
        "def precision_recall_f1(pred, gold):\n",
        "    if not pred and not gold:\n",
        "        return (1.0, 1.0, 1.0)\n",
        "    if not pred:\n",
        "        return (0.0, 0.0, 0.0)\n",
        "    if not gold:\n",
        "        return (0.0, 0.0, 0.0)\n",
        "\n",
        "    tp = len(pred.intersection(gold))\n",
        "    precision = tp / len(pred) if pred else 0\n",
        "    recall = tp / len(gold) if gold else 0\n",
        "    f1 = (2 * precision * recall / (precision + recall)) if (precision + recall) else 0\n",
        "    return (precision, recall, f1)\n",
        "\n",
        "def evaluate_pipeline(spider_dev, dev_gold, conn):\n",
        "    exact_matches = 0\n",
        "    execution_matches = 0\n",
        "    valid_total = 0\n",
        "\n",
        "    existing_tables = get_tables_in_db(conn)\n",
        "\n",
        "    for i in range(len(spider_dev)):\n",
        "        question = spider_dev[i][\"question\"]\n",
        "        gold_sql = dev_gold[i].strip()\n",
        "\n",
        "        state = {\n",
        "            \"question\": question,\n",
        "            \"attempts\": 0,\n",
        "            \"sql_query\": \"\",\n",
        "            \"query_result\": None,\n",
        "            \"sql_error\": False,\n",
        "            \"relevance\": \"relevant\",\n",
        "        }\n",
        "\n",
        "        # Generate SQL (your NL->SQL logic here)\n",
        "        state = convert_nl_to_sql(state)\n",
        "        gen_sql = state[\"sql_query\"].strip()\n",
        "\n",
        "        if not gen_sql:\n",
        "            continue\n",
        "\n",
        "        fixed_sql = fix_table_names_in_sql(gen_sql, existing_tables)\n",
        "        state[\"sql_query\"] = fixed_sql\n",
        "\n",
        "        if not validate_sql(fixed_sql, conn):\n",
        "            continue\n",
        "\n",
        "        state = execute_query(state, conn)\n",
        "        gen_result = state[\"query_result\"]\n",
        "        gold_result = execute_sql_direct(gold_sql, conn)\n",
        "\n",
        "        norm_gen_sql = normalize_sql(fixed_sql)\n",
        "        norm_gold_sql = normalize_sql(gold_sql)\n",
        "\n",
        "        norm_gen_result = normalize_result(gen_result)\n",
        "        norm_gold_result = normalize_result(gold_result)\n",
        "\n",
        "        exact_match = norm_gen_sql == norm_gold_sql\n",
        "        exec_match = (\n",
        "            norm_gold_result is not None and\n",
        "            norm_gen_result is not None and\n",
        "            norm_gold_result == norm_gen_result\n",
        "        )\n",
        "\n",
        "        gen_components = extract_components(norm_gen_sql)\n",
        "        gold_components = extract_components(norm_gold_sql)\n",
        "\n",
        "        select_p, select_r, select_f1 = precision_recall_f1(gen_components[\"select\"], gold_components[\"select\"])\n",
        "        from_p, from_r, from_f1 = precision_recall_f1(gen_components[\"from\"], gold_components[\"from\"])\n",
        "        where_p, where_r, where_f1 = precision_recall_f1(gen_components[\"where\"], gold_components[\"where\"])\n",
        "\n",
        "        select_jaccard = jaccard_similarity(gen_components[\"select\"], gold_components[\"select\"])\n",
        "        from_jaccard = jaccard_similarity(gen_components[\"from\"], gold_components[\"from\"])\n",
        "        where_jaccard = jaccard_similarity(gen_components[\"where\"], gold_components[\"where\"])\n",
        "\n",
        "        overall_str_sim = string_similarity(norm_gen_sql, norm_gold_sql)\n",
        "\n",
        "        valid_total += 1\n",
        "        if exact_match:\n",
        "            exact_matches += 1\n",
        "        if exec_match:\n",
        "            execution_matches += 1\n",
        "\n",
        "        print(f\"\\n--- Example {valid_total} ---\")\n",
        "        print(f\"Question:          {question}\")\n",
        "        print(f\"Gold SQL:   {norm_gold_sql}\")\n",
        "        print(f\"Generated SQL (norm):    {norm_gen_sql}\")\n",
        "        print(f\"Exact Match:       {exact_match}\")\n",
        "        print(f\"Exec Match:        {exec_match}\")\n",
        "        print(f\"Execution Result:  {norm_gold_result}\")\n",
        "        print(\"Component F1 Scores:\")\n",
        "        print(f\"  SELECT - P: {select_p:.2f}, R: {select_r:.2f}, F1: {select_f1:.2f}\")\n",
        "        print(f\"  FROM   - P: {from_p:.2f}, R: {from_r:.2f}, F1: {from_f1:.2f}\")\n",
        "        print(f\"  WHERE  - P: {where_p:.2f}, R: {where_r:.2f}, F1: {where_f1:.2f}\")\n",
        "        print(\"Component Jaccard Similarity:\")\n",
        "        print(f\"  SELECT - {select_jaccard:.2f}\")\n",
        "        print(f\"  FROM   - {from_jaccard:.2f}\")\n",
        "        print(f\"  WHERE  - {where_jaccard:.2f}\")\n",
        "        print(f\"Overall SQL String Similarity (SeqMatch): {overall_str_sim:.2f}\")\n",
        "\n",
        "    print(\"\\n=== Evaluation Summary ===\")\n",
        "    print(f\"Valid examples:           {valid_total}\")\n",
        "    print(f\"Exact match count:        {exact_matches}\")\n",
        "    print(f\"Execution match count:    {execution_matches}\")\n",
        "    if valid_total > 0:\n",
        "        print(f\"Exact match accuracy:     {exact_matches / valid_total:.2f}\")\n",
        "        print(f\"Execution match accuracy: {execution_matches / valid_total:.2f}\")\n",
        "    else:\n",
        "        print(\"No valid examples to evaluate.\")"
      ],
      "metadata": {
        "id": "L6GZKXQHDHdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dev set\n",
        "spider_dev = load_spider_dev(\"/content/dev.json\")\n",
        "\n",
        "# Load gold SQL\n",
        "dev_gold = []\n",
        "with open('/content/dev_gold.sql', 'r') as f:\n",
        "    for line in f:\n",
        "        sql = line.strip().split('\\t')[0]\n",
        "        dev_gold.append(sql)\n",
        "\n",
        "# Filter by db_id (from dev.json) and level\n",
        "target_db = \"employee_hire_evaluation\"\n",
        "target_level = \"easy\"  # change to e.g., \"easy\", \"medium\", \"hard\"\n",
        "\n",
        "filtered_dev = []\n",
        "filtered_gold = []\n",
        "\n",
        "for ex, sql in zip(spider_dev, dev_gold):\n",
        "    if ex.get(\"db_id\", \"\").lower() == target_db and ex.get(\"type\", \"\").lower() == target_level:\n",
        "        filtered_dev.append(ex)\n",
        "        filtered_gold.append(sql)\n",
        "\n",
        "print(f\"Filtered {len(filtered_dev)} examples from database '{target_db}' with level '{target_level}'.\")\n",
        "\n",
        "# set number of examples to run (will skip ones with validation errors)\n",
        "n = 50\n",
        "filtered_dev = filtered_dev[:n]\n",
        "filtered_gold = filtered_gold[:n]\n",
        "\n",
        "# Connect to the specific database\n",
        "conn = sqlite3.connect('/content/employee_hire_evaluation.sqlite')\n",
        "\n",
        "# Run evaluation\n",
        "evaluate_pipeline(filtered_dev, filtered_gold, conn)\n",
        "\n",
        "conn.close()"
      ],
      "metadata": {
        "id": "z8-gl7OHNzLP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}